---
title: Тонкая настройка операционной системы
readPrev:
  to: /path/to/prev
  label: Предыдущий раздел
nextRead:
  to: /path/to/next
  label: Следующий раздел
---

# Тонкая настройка операционной системы

::AppAnnotation
Настоящий раздел содержит рекомендации по тонкой настройке ОС для обеспечения стабильной и безопасной работы Комплекса в производственной среде. Описанные меры включают оптимизацию параметров ядра, конфигурацию сетевой подсистемы, настройку изоляции и безопасности контейнеров, шифрование передаваемых данных, управление доступом через брандмауэр, а также организацию логирования.
::

::AppAnnotation
Соблюдение приведённых ниже рекомендаций позволяет повысить отказоустойчивость Комплекса, минимизировать сетевые задержки, обеспечить надёжную защиту данных и упростить диагностику при эксплуатации.
::

## 1.1 Рекомендованные настройки производительности

## 1.2 Инструменты настройки ядра

::AppAnnotation
Для управления параметрами ядра Linux в реальном времени и при запуске ОС применяются различные инструменты. Их использование позволяет упростить администрирование и повысить предсказуемость поведения компонентов Комплекса.
::

::AppAnnotation
В пунктах 5.1.1.1-5.1.1.3 рассмотрены доступные утилиты и инструменты для настройки параметров ядра с примерами их эффективного использования. Использование этих инструментов способствует стабильной и эффективной работе компонентов Комплекса в продуктивной среде.
::

## 1.3 sysctl

sysctl – основная утилита для чтения и изменения параметров ядра. Она взаимодействует с каталогом /proc/sys/, где доступны все системные параметры.

В качестве примера настройки параметров утилитой, чтобы изменить параметр net.core.somaxconn, который отвечает за максимальное число соединений в очереди для принятия сокетом, можно использовать следующую команду:

sysctl -w net.core.somaxconn=1024

Для сохранения настроек нужно добавить строку в файл /etc/sysctl.conf (или создать отдельный файл в /etc/sysctl.d/) со следующим содержанием:

net.core.somaxconn = 1024

## 1.4 Tuned

Tuned – утилита для применения готовых и пользовательских профилей оптимизации и динамической адаптивной настройки ОС. Она включает профили, ориентированные на разные типы нагрузки (например, throughput-performance, latency-performance).

Установка утилиты производится командами:

yum install tuned # Для ОС на основе RHEL

apt-get install tuned # Для ОС на основе Debian

Для активации профиля нужно ввести:

tuned-adm profile throughput-performance

Процесс создания пользовательского профиля для Kubernetes состоит из выполнения следующих команд:

создать каталог для профиля:

mkdir /etc/tuned/kubernetes-optimize

создать файл tuned.conf со следующими настройками:

[main]

include=throughput-performance

[sysctl]

net.ipv4.ip_local_port_range = 10240 65535

net.core.somaxconn = 1024

net.ipv4.tcp_max_syn_backlog = 2048

активировать профиль:

tuned-adm profile kubernetes-optimize

ktune

Утилита ktune из пакета tuned применяет пользовательские параметры настройки ОС при запуске на основе выбранного профиля. Хотя ktune реже используется в современных дистрибутивах, понимание его роли помогает оценить эволюцию автоматических решений для настройки ОС.

## 1.5 Дополнительные утилиты

Утилиты numactl и numad используются для ОС с неравномерным доступом к памяти (NUMA). Эти инструменты управляют политиками распределения памяти и привязкой к процессорам, что особенно важно для приложений, интенсивно использующих память.

Утилита cpufrequtils позволяет управлять политиками масштабирования частоты процессора, что помогает оптимизировать энергопотребление и производительность в зависимости от характеристик нагрузки.

Демон irqbalance распределяет аппаратные прерывания между процессорами в многопроцессорной операционной системе. Это повышает производительность для приложений с высокой интенсивностью ввода-вывода, избегая перегрузки одного процессора.

## 1.6 Сетевые параметры

Для повышения производительности сетевой подсистемы ОС рекомендуется изменить ряд параметров ядра Linux. Настройка данных параметров позволяет снизить задержки при установлении соединений, расширить допустимое количество одновременных подключений и предотвратить исчерпание доступных портов при загруженном сетевом взаимодействии.

Для настройки производительности сетевых параметров в кластерах Kubernetes с высокой нагрузкой рекомендуется внести изменения в следующие параметры из таблицы 5.

Таблица 5 – Настройка сетевых параметров в кластерах Kubernetes

| Параметр | Команды |
| -------- | ------- |
| net.core.somaxconn<br>Задаёт максимальное количество соединений, которые могут находиться в очереди на прием серверным приложением. При высоких нагрузках значение по умолчанию может ограничивать число обрабатываемых подключений | Проверка текущего значения:<br>sysctl net.core.somaxconn<br>Рекомендуемое значение:<br>sysctl -w net.core.somaxconn=1024<br>Для сохранения значения после перезагрузки необходимо добавить в файл /etc/sysctl.conf строку:<br>net.core.somaxconn = 1024 |
| net.ipv4.tcp_max_syn_backlog<br>Определяет максимальное количество входящих TCP-соединений, ожидающих подтверждения. Увеличение значения улучшает поведение сервера при большом количестве одновременных попыток подключения | Проверка текущего значения:<br>sysctl net.ipv4.tcp_max_syn_backlog<br>Рекомендуемое значение:<br>sysctl -w\ net.ipv4.tcp_max_syn_backlog=2048<br>Для сохранения после перезагрузки:<br>net.ipv4.tcp_max_syn_backlog = 2048 |
| net.ipv4.ip_local_port_range<br>Определяет диапазон локальных портов, используемых для установления исходящих подключений. Расширение диапазона позволяет избежать исчерпания доступных портов при интенсивной сетевой активности | Проверка текущего диапазона:<br>sysctl net.ipv4.ip_local_port_range<br>Рекомендуемый диапазон:<br>sysctl -w\ net.ipv4.ip_local_port_range="10240\ 65535"<br>Для постоянного применения:<br>net.ipv4.ip_local_port_range = 10240 65535 |

## 1.7 Мониторинг и корректировка параметров

После применения настроек рекомендуется контролировать их влияние на производительность ОС. Для анализа состояния сетевых соединений и очередей можно использовать следующие утилиты:

netstat — отображение активных подключений и статистики по протоколам;

ss — более современный аналог netstat с поддержкой фильтрации;

ipvsadm — просмотр состояния балансировщиков при использовании IP Virtual Server.

::AppAnnotation
Рекомендуется производить изменения параметров поэтапно, контролируя метрики нагрузки и стабильности работы. Оптимальные значения могут варьироваться в зависимости от характеристик оборудования и типа размещения Комплекса.
::

## 1.8 Управление памятью

Эффективное управление оперативной памятью критически важно для обеспечения стабильной работы узлов, особенно в условиях высокой нагрузки и большого количества запущенных контейнеров. ОС Linux предоставляет ряд параметров, позволяющих гибко управлять распределением памяти и поведением ОС при её нехватке.

Для настройки параметров управления памятью рекомендуется использовать значения, приведённые в таблице 6**Ошибка! Источник ссылки не найден.**.

Таблица 6 – Рекомендуемые параметры управления памятью

| Параметр | Команды |
| -------- | ------- |
| vm.swappiness<br>Управляет склонностью ядра использовать swap. Высокое значение указывает на склонность использовать swap даже при наличии свободной оперативной памяти, тогда как низкое значение заставляет ядро избегать свопирования. Для узлов Kubernetes часто рекомендуется более низкое значение, чтобы приложения оставались в оперативной памяти для более быстрого доступа | Проверка текущего значения:<br>sysctl vm.swappiness<br>Чтобы уменьшить использование swap, можно установить более низкое значение, например 10, что поможет сохранить больше данных в физической памяти:<br>sysctl -w vm.swappiness=10<br>Чтобы изменения сохранялись после перезагрузки, следует добавить следующую строку в файл /etc/sysctl.conf: <br>vm.swappiness = 10 |
| vm.overcommit_memory<br>Определяет политику распределения памяти при превышении её доступного объёма.<br>Существуют три настройки параметра: <br>0 — ядро использует эвристическую оценку;<br>1 — разрешает переполнение памяти без ограничений;<br>2 — запрещает запросы, если памяти и swap недостаточно.<br>Для узлов Kubernetes значение 1 может быть полезным, если рабочие нагрузки требуют много памяти, но фактически используют только ее часть | Проверка текущего значения:<br>sysctl vm.overcommit_memory<br>Для разрешения переполнения памяти можно установить значение 1:<br>sysctl -w vm.overcommit_memory=1<br>Чтобы изменения сохранялись после перезагрузки, следует добавить следующую строку в файл /etc/sysctl.conf: <br>vm.overcommit_memory = 1 |
| vm.dirty_ratio<br>Определяет максимальный процент оперативной памяти, который может быть заполнен "грязными" страницами (страницами, которые изменены и еще не записаны на диск) перед тем, как процессы будут вынуждены самостоятельно записывать эти данные | Проверка текущего значения:<br>sysctl vm.dirty_ratio<br>Для балансировки между использованием памяти и дискового ввода-вывода можно установить значение 15: <br>sysctl -w vm.dirty_ratio=15<br>Чтобы изменения сохранялись после перезагрузки, следует добавить следующую строку в файл /etc/sysctl.conf: <br>vm.dirty_ratio = 15 |
| vm.dirty_background_ratio<br>Задает процент памяти, при котором ядро начинает асинхронно записывать "грязные" страницы на диск, чтобы избежать чрезмерного накопления таких страниц | Проверка текущего значения:<br>sysctl vm.dirty_background_ratio<br>Для снижения задержек записи данных на диск можно установить значение 5: <br>sysctl -w vm.dirty_background_ratio=5<br>Чтобы изменения сохранялись после перезагрузки, следует добавить следующую строку в файл /etc/sysctl.conf: <br>vm.dirty_background_ratio = 5 |

Все настройки необходимо применять с учётом объема доступной памяти и характера нагрузок на узлы, проводя предварительное тестирование в пилотной среде.

## 1.9 Оптимизация файловой системы и дискового ввода-вывода

::AppAnnotation
Корректная настройка параметров работы с файловой системой и вводом-выводом позволяет обеспечить стабильную работу компонентов Комплекса, интенсивно взаимодействующих с хранилищем данных. Это особенно актуально для компонентов, работающих с журналируемыми данными, логами и временными файлами.
::

Рекомендуется выполнить настройку параметров, указанных в таблице 7.

Таблица 7 – Параметры файловой системы и ввода-вывода

| Параметр | Команды |
| -------- | ------- |
| fs.file-max<br>Устанавливает максимальное количество файловых дескрипторов, которые может выделить ядро. Это особенно важно для узлов Kubernetes, на которых запускается множество контейнеров или приложений, открывающих много файлов одновременно | Проверка текущего значения: <br>sysctl fs.file-max<br>Увеличить лимит, например, до 500000: <br>sysctl -w fs.file-max=500000<br>Чтобы изменение сохранилось после перезагрузки, следует добавить строку в файл /etc/sysctl.conf: <br>fs.file-max = 500000 |
| fs.inotify.max_user_watches<br>Определяет максимальное количество файлов, которые могут отслеживаться на изменения с помощью inotify. Это особенно важно для приложений, реагирующих на изменения в режиме реального времени, таких как инструменты live-reload или сервисы синхронизации файлов | Проверка текущего значения: <br>sysctl fs.inotify.max_user_watches  <br>Увеличить лимит, например, до 524288: <br>sysctl -w\ fs.inotify.max_user_watches=524288  <br>Чтобы изменение сохранилось после перезагрузки, следует добавить строку в файл /etc/sysctl.conf: <br>fs.inotify.max_user_watches = 524288 |
| Настройка планировщика ввода-вывода (I/O Scheduler)<br>Выбор подходящего планировщика I/O влияет на производительность операций ввода-вывода, включая пропускную способность, задержки и число операций в секунду (IOPS). Для большинства случаев рекомендуется использовать deadline или mq-deadline | Проверка текущего планировщика для устройства (например, /dev/sda):<br>cat /sys/block/sda/queue/scheduler  <br>Изменить планировщик на deadline для снижения задержек: <br>echo deadline >\ /sys/block/sda/queue/scheduler<br>Для систем с SSD или высокопроизводительными накопителями планировщики noop или mq-deadline могут предложить лучшую производительность за счет простоты и меньшей нагрузки |

Рекомендуется использовать соответствующие udev-правила или скрипты инициализации, чтобы выбранный планировщик применялся автоматически при запуске ОС.

## 1.10 Мониторинг и корректировка параметров ввода-вывода

::AppAnnotation
После применения настроек файловой системы и параметров дискового ввода-вывода (см. раздел 5.1.5) необходимо контролировать их влияние на производительность. Для оценки используются стандартные инструменты мониторинга, такие как:
::

iostat — анализирует статистику по дисковому вводу-выводу;

iotop — отслеживает текущую активность ввода-вывода процессов;

sar —собирает и отображает статистику производительности различных подсистем ОС.

::AppAnnotation
Кроме того, Kubernetes предоставляет встроенные метрики и журналы, позволяющие анализировать I/O-паттерны приложений. Это обеспечивает возможность адаптировать параметры под реальные рабочие нагрузки и добиться устойчивой производительности компонентов Комплекса.
::

## 1.11 Планирование процессов

Параметры планировщика процессов ядра Linux оказывают прямое влияние на производительность и отклик приложений в среде Kubernetes, особенно в условиях высокой конкуренции за ресурсы CPU. Рекомендуется внести следующие настройки в параметры ядра, указанные в таблице 8.

Таблица 8 – Параметры настройки планирования процессов

| Параметр | Команды |
| -------- | ------- |
| kernel.sched_migration_cost_ns<br>Определяет минимальное время (в наносекундах) выполнения процесса на текущем CPU до его возможной миграции на другой CPU. Это важно для среды Kubernetes, где поды могут часто перемещаться между CPU. Понижение значения может сделать планировщик более агрессивным в перемещении процессов, улучшая балансировку нагрузки, но с риском увеличения числа промахов кеша | Проверка текущего значения: <br>sysctl kernel.sched_migration_cost_ns<br>Для повышения отзывчивости можно установить значение, например, 500000 (0,5 мс) <br>sysctl -w\ kernel.sched_migration_cost_ns=500000<br>Чтобы изменение сохранилось после перезагрузки, следует добавить строку в файл /etc/sysctl.conf: <br>kernel.sched_migration_cost_ns=500000 |
| kernel.sched_autogroup_enabled<br>Включает функцию автоматической группировки задач с похожими шаблонами выполнения для улучшения отзывчивости ОС. Эта настройка полезна для настольных систем, но в серверной среде, особенно при работе с Kubernetes, такое поведение может привести к неравномерному распределению ресурсов CPU между подами | Для проверки, включена ли автогруппировка: <br>sysctl kernel.sched_autogroup_enabled <br>Для равномерного распределения ресурсов CPU можно отключить функцию: <br>sysctl -w\ kernel.sched_autogroup_enabled=0<br>Чтобы изменение сохранилось после перезагрузки, следует добавить строку в файл /etc/sysctl.conf:<br>kernel.sched_autogroup_enabled=0 |

Применение вышеуказанных параметров требует предварительного тестирования в условиях, приближенных к промышленной среде, и регулярного анализа нагрузки на CPU.

## 1.12 Привязка к процессору и управление CPU

Для повышения производительности и предсказуемости работы критически важных компонентов в составе Портала разработчика может применяться привязка контейнеров к определённым процессорным ядрам (CPU pinning), а также управление распределением ресурсов процессора с использованием средств Kubernetes.

Конфигурации развертывания Kubernetes описываются в pod – наименьшей единице развертывания в Kubernetes, задаются в формате YAML (реже — JSON). Параметры pod определяют, какие контейнеры запускаются и какие ресурсы они используют, какие политики применяются и как организовано взаимодействие с другими элементами ОС.

Kubernetes предоставляет следующие механизмы управления ресурсами CPU:

привязка контейнеров к конкретным процессорам (CPU pinning);

использование наборов процессоров (CPU sets);

указание запросов и ограничений на использование CPU (параметры requests и limits в спецификации pod).

Такие настройки особенно актуальны для компонентов, чувствительных к задержкам или требующих высокой вычислительной мощности, поскольку позволяют:

исключить миграцию процессов между ядрами;

повысить эффективность кеширования;

сократить накладные расходы на переключение контекста;

обеспечить размещение модулей ближе к необходимым узлам NUMA.

Пример спецификации pod с указанием ресурсов и узловой привязки:

apiVersion: v1

kind: Pod

metadata:

name: cpu-affinity-example

spec:

containers:

- name: container1
    image: nginx

resources:

requests:

cpu: "2"

limits:

cpu: "4"

affinity:

nodeAffinity:

requiredDuringSchedulingIgnoredDuringExecution:

nodeSelectorTerms:

- matchExpressions:
- key: kubernetes.io/e2e-az-name
            operator: In

values:

- e2e-az1
- e2e-az2
Данный фрагмент демонстрирует задание минимального (requests) и максимального (limits) количества CPU для контейнера, а также настройку привязки к определённым зонам доступности. Это позволяет Kubernetes запланировать выполнение пода на узле, соответствующем заданным условиям, и тем самым обеспечить требуемый уровень производительности.

## 1.13 Мониторинг использования CPU и корректировка

Для анализа нагрузки на CPU применяются как стандартные утилиты, так и специализированные средства мониторинга в рамках Kubernetes:

htop, mpstat — отслеживают использование процессоров и процессов;

Prometheus и Grafana — используются для сбора и визуализации метрик кластера.

Регулярный мониторинг позволяет выявить узкие места и провести корректировку конфигурации, учитывая характеристики архитектуры и тип рабочих нагрузок.

## 1.14 Рекомендованные настройки безопасности

## 1.15 Изоляция процессов и данных

::AppAnnotation
Для обеспечения безопасности и отказоустойчивости все компоненты Комплекса запускаются в отдельных контейнерах. Это обеспечивает изоляцию процессов и снижает риск того, что сбой или компрометация одного из компонентов повлияет на работу остальных.
::

Для повышения уровня безопасности рекомендуется выполнять следующие настройки по изоляции контейнеров:

::AppAnnotation
создавать изолированную внутреннюю сеть Docker (bridge), в которой взаимодействуют только контейнеры Комплекса;
::

ограничивать сетевой доступ к каждому контейнеру, разрешая только необходимые порты:

PostgreSQL: доступ разрешён только из контейнеров, выполняющих PHP-код;

Redis: доступен только из контейнера с приложением;

устанавливать лимиты на использование ресурсов (CPU и память) для каждого контейнера, чтобы предотвратить избыточное потребление и влияние одного контейнера на другие.

## 1.16 Шифрование

Передача данных между пользователем и сервером осуществляется по защищённым TLS-соединениям. Рекомендуется использовать следующие настройки:

все входящие соединения обрабатываются по протоколу TLS версии 1.2 или 1.3;

TLS-сертификаты настраиваются и обновляются автоматически через Traefik;

в незашифрованный порт (80) осуществляется только перенаправление на HTTPS (порт 443).

## 1.17 Брандмауэр

Для защиты сетевой инфраструктуры следует настроить брандмауэр следующим образом:

разрешить только следующие порты:

80/tcp — перенаправление HTTP-запросов на HTTPS;

443/tcp — основной порт для защищённого взаимодействия с системой;

все остальные порты должны быть закрыты. Попытки соединения через другие порты должны блокироваться.

## 1.18 Логирование

::AppAnnotation
Система логирования настраивается для всех компонентов Комплекса с целью обеспечения наблюдаемости и возможности оперативного реагирования на возникающие ошибки.
::

На уровне приложения логирование конфигурируется через переменную окружения LOG_CHANNEL. Поддерживаются следующие режимы:

stack / single — непрерывная запись логов в один файл;

daily — создание отдельного файла для каждого дня; файлы хранятся до 14 суток;

syslog — отправка логов в системный журнал;

null — отключение логирования.

::AppAnnotation
Логи приложения сохраняются в каталоге storage/logs. Кроме того, все записи логов доступны для просмотра в административной панели CMS в разделе "Система → Журнал событий" (/backend/system/eventlogs).
::